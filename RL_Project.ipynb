{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Danny's Lunar Lander"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this project, I chose to look at OpenAI's gym environment called LunarLander-v2 which simulates a very basic 2D spacecraft. The goal is to land the spacecraft in between the flags at an appropriate orientation and speed. In the spirit of the most recent unit, I looked into using a DQN to solve this and upon further research, I found out that it would be well suited for this problem. I used OpenAI's Gym to interact with the environment, Keras to create my model, and numpy for various computational tasks.\n",
    "\n",
    "### Environment Setup\n",
    "This agent is a Lunar spacecraft with 4 possible actions. It can either fire the right-facing thruster, the left-facing thruster, the bottom-facing thuster or no thruster at all. These actions correspond to the integers in the range 0-3 inclusive. We will call that the action space. The state space is a big larger and more involved. The state vector that is returned when we take an observation is of length 8 and follows the following format: [x_coord, y_coord, x_velocity, y_velocity, lander_angle, angular_velocity, isLeftLegDown, isRightLegDown]. At first I was intimidated by this vector and thought that there might have ben a lot of math involved, but reinforcement learning takes care of that for us. The end goal of this environment is to land on the ground below in between the two flags. The reward system is as follows. You get 100-140 points for moving from the top of the screen down to the ground, and points are lost if the lander moves up towards the sky again. If you land and come to rest, you get 100 points but if you crash, that is -100 points. Firing the main engine loses you 0.3 points per frame that it is fired so there is incentive to finish quickly, and 10 points are awarded for each leg making contact with the ground. \n",
    "\n",
    "### Model Setup\n",
    "To begin, I recalled the lecture where we talked about DQN's. In order to solve the problem of correlated samples, a DQN uses a replay buffer to store information that it can sample from randomly. I created a class called ReplayBuffer that allowed me to store this information and create easily callable functions to add to and sample from it. \n",
    "\n",
    "Next, I used Keras to create a target neural network using the Sequential() model. The architecture of my network was influenced by my research but also somewhat of a guess and check. I settled on 4 hidden layers with an input dimension of 8 which represents an observation vector. Each hidden layer uses relu as it's activation function and the output layer has a linear activation function because I need to have multiple outputs. The output layer has an output dimension of size 4 because that is the size of the action space. I chose a learning rate of 0.001 as per the suggestion in previous lectures to keep it small in on the order of 0.001.\n",
    "\n",
    "I also use a greedy-epsilon approach starting with an epsilon of 1 and decreasing the epsilon by a small amount each iteration. This works by randomly generating a number between 0 and 1. If the number is below epsilon, then I take a random sample of the action space and use that as my action. Otherwise, I use my network to predict an action and use that. This helps the model still explore even after it has been trained. \n",
    "\n",
    "### Training Procedure\n",
    "In the training process, I let my model run 400 episodes and have a maximum iteration limit on each episode to avoid infinite hovering. The model begins with terrible performance and as is the way with reinforcement learning, it slowly begins to perform better. This model doesn't do particularly well until around 80-100 episodes. At this point, it begins to land more consistently and gets closeer to the desired landing pad. The model takes about 2 hours to run fully so it was a bit of a pain to test it because it is hard to tell if it is learning properly or not. \n",
    "\n",
    "### Troubleshooting\n",
    "There has been PLENTY of troubleshooting! First, just getting the data formatted correctly took my some time. Initially in my replay buffer, I stored it as a 2D numpy array and that allowed for quick slicing and indexing when sampling. However, that gave me data dimension troubles that simply weren't worth the amount of spaghetti code necessary to fix them. So I reverted back to storing it as a list of lists which I then generate random indices for and then loop through my sample to get my desired numpy arrays of states, rewards, etc.\n",
    "\n",
    "Additionally, in my training loop, I forgot to set my current state equal to the successor state at the end of my iteration which resulted in my rocket starting off random and learning to engage only the right or left engine. This caused the lander to begin doing flips the whole way down which, while that's pretty cool, it was not efficient nor the purpose of this exercise. It took my awhile to fix that because it was such a minor bug to notice in my code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n",
    "At the beginning of training, it is all random actions being chosen so we see behavior like this:  \n",
    "\n",
    "![SegmentLocal](RL_Learning_Gif_2.gif \"segment\")\n",
    "\n",
    "As the lander acts like this and recieved large negative rewards, it learns that it shouldn't be doing this and weights are adjusted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But later after about 200 episodes, we see the lander aiming itself and learning how to land. This is about as good as I got with my model.  \n",
    "\n",
    "![SegmentLocal](Project_gif_2.gif \"segment\")\n",
    "\n",
    "And after 300 episodes:  \n",
    "\n",
    "![SegmentLocal](LunarLanderGif4.gif \"segment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion and Results\n",
    "\n",
    "It seems that my model did work, however I would love to experiment more with it. For instance, I believe that changing the architecture of the network would be interesting. Perhaps I would add or subtract a hidden layer as well as change the sizes of the hidden layers. I also would like to look into using Huber loss instead of MSE in my model because based on what I found online, it also seems like an appropriate loss function. \n",
    "\n",
    "My behavior took around 200 episodes to reall =y see success and I wonder what I could change that would allow me to see a faster convergence. I would also like to reimplement my ReplayBuffer with the Numpy 2D array, as I think that would speed up the training process at least with faster computation.\n",
    "\n",
    "Overall, I think that this was an interesting project and I appreciate OpenAI's efforts to making reinforcement learning environments accessible. I learned a lot about RL in the process of completing this project and I hope to continue on with this project to learn even more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORIGINAL REPLAY BUFFER USING NUMPY THAT DID NOT WORK\n",
    "# class ReplayBuffer():\n",
    "\n",
    "#     def __init__(self):\n",
    "\n",
    "#         self.memory = np.array([[[0,0,0,0,0,0,0,0],0,0,[0,0,0,0,0,0,0,0],0]])\n",
    "#         self.currSize = 0\n",
    "\n",
    "#     def sample_from_buffer(self, batch_size):\n",
    "\n",
    "#         if self.currSize < batch_size:\n",
    "#             return None, None, None, None, None\n",
    "\n",
    "#         currentSample = self.memory[np.random.choice(self.memory.shape[0], batch_size, replace=False)]\n",
    "\n",
    "#         states = currentSample[:,0]\n",
    "#         actions = currentSample[:,1]\n",
    "#         rewards = currentSample[:,2]\n",
    "#         succs = currentSample[:,3]\n",
    "#         dones = currentSample[:,4]\n",
    "\n",
    "\n",
    "#         return states, actions, rewards, succs, dones\n",
    "\n",
    "#     def add_to_buffer(self, state, action, reward, successor, done):\n",
    "\n",
    "#         if self.currSize == 0:\n",
    "#             self.memory[0,:] = (state, action, reward, successor, done)\n",
    "#         else:\n",
    "#             self.memory = np.vstack([self.memory,(state, action, reward, successor, done)])\n",
    "#         self.currSize += 1\n",
    "\n",
    "#     def get_size(self):\n",
    "#         return self.currSize\n",
    "\n",
    "#     def print_buffer(self):\n",
    "#         [print(i) for i in self.memory]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.memory = []\n",
    "        self.currSize = 0\n",
    "\n",
    "    def sample_from_buffer(self, batch_size):\n",
    "        # If the size of memory is not enough to fill a batch, then we return Nones\n",
    "        if self.currSize < batch_size:\n",
    "            return None, None, None, None, None\n",
    "\n",
    "        # Take sample from our memory of size batch_size\n",
    "        currentSample = random.sample(self.memory, batch_size)\n",
    "\n",
    "        # Initialize all of our lists\n",
    "        s, a, r, s_prime, d = [],[],[],[],[]\n",
    "        \n",
    "        # Fill them with the appropriate data\n",
    "        for samp in currentSample:\n",
    "\n",
    "            s.append(samp[0])\n",
    "            a.append(samp[1])\n",
    "            r.append(samp[2])\n",
    "            s_prime.append(samp[3])\n",
    "            d.append(samp[4])\n",
    "\n",
    "            # Turn all of these lists into numpy arrays\n",
    "            states, actions, rewards, succs, dones = np.array(s), np.array(a), np.array(r), np.array(s_prime), np.array(d)\n",
    "\n",
    "        # Remove single-dimensional entries\n",
    "        states = np.squeeze(states)\n",
    "        succs = np.squeeze(succs)\n",
    "\n",
    "        return states, actions, rewards, succs, dones\n",
    "\n",
    "    def add_to_buffer(self, state, action, reward, successor, done):\n",
    "        \n",
    "        # Adding one entry which holds all of the data associated with a single step\n",
    "        self.memory.append((state, action, reward, successor, done))\n",
    "        # Increment size to keep track of it\n",
    "        self.currSize += 1\n",
    "\n",
    "    def get_size(self):\n",
    "        # Returns size of memory\n",
    "        return self.currSize\n",
    "\n",
    "    def print_buffer(self):\n",
    "        # Prints each row from memory\n",
    "        [print(i) for i in self.memory]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyEpsilonWrapper(gym.ActionWrapper):\n",
    "\n",
    "    def __init__(self, env, model, epsilon):\n",
    "        super(GreedyEpsilonWrapper, self).__init__(env)\n",
    "        self.epsilon = epsilon\n",
    "        self.model = model\n",
    "        self.env = env\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \n",
    "        # Use epsilon-greedy approach, with probability epsilon, choose a random action\n",
    "        if np.random.uniform() <= self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        \n",
    "        else:\n",
    "            # Otherwise, predict our action\n",
    "            acts = self.model.predict(state)\n",
    "            return np.argmax(acts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_Agent():\n",
    "\n",
    "    def __init__(self, actions, states):\n",
    "        # Initialize all of our relevant parameters and environment\n",
    "        self.gamma = 0.993\n",
    "        self.epsilon = 1.0\n",
    "        self.replay_buf = ReplayBuffer()\n",
    "        self.num_actions = actions\n",
    "        self.num_states = states\n",
    "        self.DQN_model = self.create_model()\n",
    "        self.env=gym.make('LunarLander-v2')\n",
    "        \n",
    "        \n",
    "    def create_model(self):\n",
    "        # Create a config list which will define our NN architecture when passed to Sequential constructor\n",
    "        config = [Dense(200,input_dim=self.num_states, activation='relu'),\n",
    "                  Dense(140, activation='relu'),\n",
    "                  Dense(90, activation='relu'),\n",
    "                  Dense(self.num_actions, activation='linear')]\n",
    "        # Create model\n",
    "        model = Sequential(config)\n",
    "        \n",
    "        # Specify Adam as the optimizer with a specified learning rate\n",
    "        opt = Adam(learning_rate=0.001)\n",
    "        \n",
    "        # Compile the model with Mean Squared Error as the loss function and Adam as the optimizer\n",
    "        model.compile(loss='mse', optimizer=opt)\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def eps_decrease(self):\n",
    "        \n",
    "        # Decrease epsilon until a minimum epsilon\n",
    "        if self.epsilon > 0.01:\n",
    "            self.epsilon *= 0.996\n",
    "\n",
    "    def run_target_network(self):\n",
    "        \n",
    "        # If there isn't enough data for a batch, return\n",
    "        if self.replay_buf.get_size() < 64:\n",
    "            return\n",
    "        \n",
    "        # Sample from my Replay Buffer using sample method defined in above class\n",
    "        states, actions, rewards, succs, isDones = self.replay_buf.sample_from_buffer(64)\n",
    "\n",
    "        # If isDones is None, this means there wasn't enough data to sample\n",
    "        if isDones is None:\n",
    "            return\n",
    "\n",
    "        # Find targets by predicting on current batch of states with target network\n",
    "        all_targs = self.DQN_model.predict_on_batch(states)\n",
    "        \n",
    "        # Modify batch targets with target updates \n",
    "        all_targs[:64, [actions]] = rewards + self.gamma*(np.amax(self.DQN_model.predict_on_batch(succs), axis=1))*(1-isDones)\n",
    "\n",
    "        # Fit model on updated targets (Hence the \"moving target\" problem)\n",
    "        self.DQN_model.fit(states, all_targs, epochs=1, verbose=0)\n",
    "        \n",
    "        # Decrease epsilon so that less random actions are taken\n",
    "        self.eps_decrease()\n",
    "\n",
    "    def take_action(self, state):\n",
    "        # Instantiate a GreedyEpsilonWrapper which helps us use epsilon-greedy approach\n",
    "        choices = GreedyEpsilonWrapper(self.env, self.DQN_model, self.epsilon)\n",
    "        \n",
    "        # Return choice\n",
    "        return choices.get_action(state)\n",
    "     \n",
    "    def fix_shape(self,state):\n",
    "        \n",
    "        # Modify shape of input state to be appropriate \n",
    "        state = np.reshape(state,(1,8))\n",
    "        \n",
    "        return state\n",
    "\n",
    "    def train_model(self, num_episodes):\n",
    "        \n",
    "        # Initialize scores list\n",
    "        scores = []\n",
    "        \n",
    "        # Loop over number of episodes\n",
    "        for episode in range(num_episodes):\n",
    "\n",
    "            # Reset environment for new episode\n",
    "            state = self.env.reset()\n",
    "            \n",
    "            # Fix shape of array so that (8,) turns into (1,8)\n",
    "            state = self.fix_shape(state)\n",
    "            \n",
    "            # Run set amount of episodes in environment\n",
    "            total = self.run_episode(state, 3000)\n",
    "            scores.append(total)\n",
    "            print('Episode: ', episode)\n",
    "            print('Score: ', total, '\\n')\n",
    "\n",
    "    def run_episode(self, state, max_iters):\n",
    "        # Initialize total and count\n",
    "        total = 0\n",
    "        count = 0\n",
    "        \n",
    "        # Loop until we hit max_iters\n",
    "        while count < max_iters:\n",
    "\n",
    "            # Get an action\n",
    "            action = self.take_action(state)\n",
    "            \n",
    "            # Render the environment\n",
    "            self.env.render()\n",
    "            \n",
    "            # Get successor state, reward, done, and info dictionary by calling step with my action\n",
    "            succ, reward, isDone, info = self.env.step(action)\n",
    "            # Fix shape of successor state vector\n",
    "            succ = np.reshape(succ, (1,8))\n",
    "\n",
    "            # Add all state information to memory\n",
    "            self.replay_buf.add_to_buffer(state[0], action, reward, succ[0], isDone)\n",
    "\n",
    "            # Update this episode's score\n",
    "            total += reward\n",
    "\n",
    "            # Update state to be successor\n",
    "            state = succ\n",
    "            count += 1\n",
    "\n",
    "            # Run function to update target network\n",
    "            self.run_target_network()\n",
    "\n",
    "            # if episode is done, return\n",
    "            if isDone:\n",
    "                return total\n",
    "\n",
    "        return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPISODES = 400\n",
    "\n",
    "env = gym.make('LunarLander-v2')\n",
    "action_space = env.action_space.n\n",
    "state_space = env.observation_space.shape[0]\n",
    "\n",
    "model = DQN_Agent(action_space, state_space)\n",
    "model.train_model(NUM_EPISODES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "https://medium.com/@jonathan_hui/rl-dqn-deep-q-network-e207751f7ae4\n",
    "\n",
    "https://towardsdatascience.com/solving-lunar-lander-openaigym-reinforcement-learning-785675066197 \n",
    "\n",
    "https://github.com/openai/gym/blob/master/gym/envs/box2d/lunar_lander.py\n",
    "\n",
    "https://becominghuman.ai/beat-atari-with-deep-reinforcement-learning-part-2-dqn-improvements-d3563f665a2c\n",
    "\n",
    "https://github.com/shivaverma/OpenAIGym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
